{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d37c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "PAGES_TO_GET = 20 #100 articles per page\n",
    "pages = list(range(1,PAGES_TO_GET+1))\n",
    "main_url = 'https://www.nami.org/Blogs/NAMI-Blog'\n",
    "urls = [main_url + \"?page=\" + str(page) for page in pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39855748",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping article titles and links \n",
    "def get_article_links(url):\n",
    "    article_titles = []\n",
    "    article_links = []\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        alldivs = soup.find_all(class_='col-md-4 col-lg-3')\n",
    "        \n",
    "        for article in alldivs:\n",
    "            title = article.find('p').text\n",
    "            link = article.find('a')['href']\n",
    "            article_titles.append(title)\n",
    "            article_links.append('https://www.nami.org' + link)\n",
    "\n",
    "        return article_titles, article_links\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    \n",
    "#scraping article content\n",
    "def scrape_article_content(article_url):\n",
    "    response = requests.get(article_url)\n",
    "    if response.status_code == 200:\n",
    "        article_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extract the content of the article based on its structure\n",
    "        author = article_soup.find('p', class_='authorname').get_text().strip().replace(\"  \",\" \")\n",
    "        year = article_soup.find('span', class_='year').get_text().strip()\n",
    "        month = article_soup.find('span', class_='month').get_text().strip()\n",
    "        day = article_soup.find('span', class_='day').get_text().strip()\n",
    "        content = article_soup.find('div', class_='content-container').get_text().strip()\n",
    "    else:\n",
    "        return None, None, None, None, None\n",
    "    return author, year, month, day, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4193e67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='scraping_log.txt', level=logging.DEBUG)\n",
    "\n",
    "all_article_titles = []\n",
    "all_article_links = []\n",
    "all_article_authors = []\n",
    "all_article_years = []\n",
    "all_article_contents = []\n",
    "\n",
    "for url in urls:\n",
    "    page_article_titles, page_article_links = get_article_links(url)\n",
    "    all_article_titles.extend(page_article_titles)\n",
    "    all_article_links.extend(page_article_links)\n",
    "    sleep(3)\n",
    "\n",
    "for i, article_link in enumerate(all_article_links):\n",
    "    try:\n",
    "        article_info = scrape_article_content(article_link)\n",
    "        if article_info is not None and len(article_info) >= 5:\n",
    "            all_article_authors.append(article_info[0])\n",
    "            all_article_years.append(article_info[1])\n",
    "            all_article_contents.append(article_info[4])\n",
    "        else:\n",
    "            logging.warning(f\"Skipping article {i}: Invalid data\")\n",
    "\n",
    "        if i % 10 == 0 or i == len(all_article_links) - 1: \n",
    "            data_df = pd.DataFrame({\n",
    "                'title': all_article_titles[:len(all_article_authors)],\n",
    "                'link': all_article_links[:len(all_article_authors)],\n",
    "                'author': all_article_authors,\n",
    "                'year': all_article_years,\n",
    "                'content': all_article_contents\n",
    "            })\n",
    "            data_df.to_csv(\"article_contents_3iteration_2round.csv\", index=False)\n",
    "            logging.info(f\"Saved articles up to index {i}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error at article {i}: {e}\")\n",
    "\n",
    "    sleep(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
